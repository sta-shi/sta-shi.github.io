[{"categories":null,"content":"Update with some recent photos. 尝试开始用中文发布记录生活。 ","date":"2023-06-26","objectID":"/blog_20230626/:0:0","tags":["Blog"],"title":"2023.6.26 无聊的毕业季","uri":"/blog_20230626/"},{"categories":null,"content":"6/20 交大外滩灯光 这天20:00到20:30，花旗大楼的屏幕属于交大毕业生。所以和两个同学一起去了外滩。 进行了一些胶片风格的滤镜。 陆家嘴建筑群 地点：世贸中心外陆家嘴建筑群 \" 陆家嘴建筑群 地点：世贸中心外 陆家嘴建筑群 地点：北外滩陆家嘴建筑群 \" 陆家嘴建筑群 地点：北外滩 夜景真的很难拍，调了很久的色，感觉还算满意。 陆家嘴夜景 地点：外滩交大牛逼 \" 陆家嘴夜景 地点：外滩 其他也没什么好发的了，好想有个相机可以扫街…… ","date":"2023-06-26","objectID":"/blog_20230626/:1:0","tags":["Blog"],"title":"2023.6.26 无聊的毕业季","uri":"/blog_20230626/"},{"categories":null,"content":"This is a guide for TensorRT optimization. ","date":"2022-04-04","objectID":"/jetson_nano_tensorrt/:0:0","tags":["Article","Jetson Nano"],"title":"Jetson Nano TensorRT","uri":"/jetson_nano_tensorrt/"},{"categories":null,"content":"Overview Convert Keras Model to TensorRT\" Convert Keras Model to TensorRT Tensorflow has two versions: v1 and v2. v2 has a poor compatiblity to v1. On aurthor’s unit the code doesn’t work (some modules cannot be found), so I reinstalled Tensorflow 1.15.0 and re-trained the model (since Keras can’t operate models trained by higher vesion). Also, it’s found inconvenient to download wheel from NVIDIA download website in China. I download wheel from other device and install Tensorflow with pip3 wheel tool. Furthermore, in original code by Ardian Uman, a cap is set for TensorRT, which leads to system dump. After removing it, I can optimize Tensorflow model with TensorRT and run TensorRT model as well. In the process of performing inference, it’s found that first several inferences of TensorRT model costs a rather long period that significantly influences the result. To obtain a “seemingly reasonable” average time, I abandoned first ten results of each model inference. Reason for the weird phenomenon remains studying. ","date":"2022-04-04","objectID":"/jetson_nano_tensorrt/:1:0","tags":["Article","Jetson Nano"],"title":"Jetson Nano TensorRT","uri":"/jetson_nano_tensorrt/"},{"categories":null,"content":"Convert Keras Model to Tensorflow Model # import the needed libraries import tensorflow as tf tf.keras.backend.set_learning_phase(0) #use this if we have batch norm layer in our network from tensorflow.keras.models import load_model # path we wanna save our converted TF-model MODEL_PATH = \"./model/TensorFlow\" # load the Keras model model = load_model('./model/modelLeNet5.h5') # save the model to Tensorflow model saver = tf.train.Saver() sess = tf.keras.backend.get_session() save_path = saver.save(sess, MODEL_PATH) print(\"Keras model is successfully converted to TF model in \"+MODEL_PATH) ","date":"2022-04-04","objectID":"/jetson_nano_tensorrt/:2:0","tags":["Article","Jetson Nano"],"title":"Jetson Nano TensorRT","uri":"/jetson_nano_tensorrt/"},{"categories":null,"content":"Use TensorRT to Optimize Tensorflow Model ","date":"2022-04-04","objectID":"/jetson_nano_tensorrt/:3:0","tags":["Article","Jetson Nano"],"title":"Jetson Nano TensorRT","uri":"/jetson_nano_tensorrt/"},{"categories":null,"content":"Convert Tensorflow Model to Frozen Model # import the needed libraries import tensorflow as tf import tensorflow.contrib.tensorrt as trt from tensorflow.python.platform import gfile # has to be use this setting to make a session for TensorRT optimization with tf.Session() as sess: # import the meta graph of the tensorflow model saver = tf.train.import_meta_graph(\"./model/TensorFlow.meta\") # then, restore the weights to the meta graph saver.restore(sess, \"./model/TensorFlow\") # specify which tensor output you want to obtain # (correspond to prediction result) your_outputs = [\"output_tensor/Softmax\"] # convert to frozen model frozen_graph = tf.graph_util.convert_variables_to_constants( sess, # session tf.get_default_graph().as_graph_def(),# graph+weight from the session output_node_names=your_outputs) #write the TensorRT model to be used later for inference with gfile.FastGFile(\"./model/frozen_model.pb\", 'wb') as f: f.write(frozen_graph.SerializeToString()) print(\"Frozen model is successfully stored!\") ","date":"2022-04-04","objectID":"/jetson_nano_tensorrt/:3:1","tags":["Article","Jetson Nano"],"title":"Jetson Nano TensorRT","uri":"/jetson_nano_tensorrt/"},{"categories":null,"content":"Optimize Frozen Model to TensorRT Graph # convert (optimize) frozen model to TensorRT model trt_graph = trt.create_inference_graph( input_graph_def=frozen_graph,# frozen model outputs=your_outputs, max_batch_size=2,# specify your max batch size max_workspace_size_bytes=2*(10**9),# specify the max workspace precision_mode=\"FP32\") # precision, can be \"FP32\" (32 floating point precision) or \"FP16\" #write the TensorRT model to be used later for inference with gfile.FastGFile(\"./model/TensorRT_model.pb\", 'wb') as f: f.write(trt_graph.SerializeToString()) print(\"TensorRT model is successfully stored!\") all_nodes = len([1 for n in frozen_graph.node]) print(\"numb. of all_nodes in frozen graph:\", all_nodes) trt_engine_nodes = len([1 for n in trt_graph.node if str(n.op) == 'TRTEngineOp']) print(\"numb. of trt_engine_nodes in TensorRT graph:\", trt_engine_nodes) all_nodes = len([1 for n in trt_graph.node]) print(\"numb. of all_nodes in TensorRT graph:\", all_nodes) ","date":"2022-04-04","objectID":"/jetson_nano_tensorrt/:3:2","tags":["Article","Jetson Nano"],"title":"Jetson Nano TensorRT","uri":"/jetson_nano_tensorrt/"},{"categories":null,"content":"Perform Inference with TensorRT Model # import the needed libraries import tensorflow as tf import tensorflow.contrib.tensorrt as trt # must import this although we will not use it explicitly from tensorflow.python.platform import gfile from PIL import Image import numpy as np import time from matplotlib import pyplot as plt # read the testing images (only for example) img1= Image.open(\"dataset/mnist/testing/0/test3_0.jpg\") img2= Image.open(\"dataset/mnist/testing/1/test2_1.jpg\") img1 = np.asarray(img1) img2 = np.asarray(img2) input_img = np.concatenate((img1.reshape((1, 28, 28, 1)), img2.reshape((1, 28, 28, 1))), axis=0) # function to read a \".pb\" model # (can be used to read frozen model or TensorRT model) def read_pb_graph(model): with gfile.FastGFile(model,'rb') as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) return graph_def # perform inference # original model FROZEN_MODEL_PATH = './model/frozen_model.pb' graph = tf.Graph() with graph.as_default(): with tf.Session() as sess: # read frozen model trt_graph = read_pb_graph(FROZEN_MODEL_PATH) # obtain the corresponding input-output tensor tf.import_graph_def(trt_graph, name='') input = sess.graph.get_tensor_by_name('input_tensor_input:0') output = sess.graph.get_tensor_by_name('output_tensor/Softmax:0') # in this case, it demonstrates to perform inference for 50 times total_time = 0; n_time_inference = 100 out_pred = sess.run(output, feed_dict={input: input_img}) for i in range(n_time_inference): t1 = time.time() out_pred = sess.run(output, feed_dict={input: input_img}) t2 = time.time() delta_time = t2 - t1 if i\u003e9: total_time += delta_time print(\"Needed time in inference \" + str(i) + \": \", delta_time) avg_time_original_model = total_time / (n_time_inference-10) print(\"Average inference time: \", avg_time_original_model) # TensorRT model TENSORRT_MODEL_PATH = './model/TensorRT_model.pb' graph = tf.Graph() with graph.as_default(): with tf.Session() as sess: # read TensorRT model trt_graph = read_pb_graph(TENSORRT_MODEL_PATH) # obtain the corresponding input-output tensor tf.import_graph_def(trt_graph, name='') input = sess.graph.get_tensor_by_name('input_tensor_input:0') output = sess.graph.get_tensor_by_name('output_tensor/Softmax:0') # in this case, it demonstrates to perform inference for 50 times total_time = 0; n_time_inference = 100 out_pred = sess.run(output, feed_dict={input: input_img}) for i in range(n_time_inference): t1 = time.time() out_pred = sess.run(output, feed_dict={input: input_img}) t2 = time.time() delta_time = t2 - t1 if i\u003e9: total_time += delta_time print(\"Needed time in inference \" + str(i) + \": \", delta_time) avg_time_tensorRT = total_time / (n_time_inference-10) print(\"Average inference time: \", avg_time_tensorRT) print(\"TensorRT improvement compared to the original model:\", avg_time_original_model/avg_time_tensorRT) plt.figure('img 1') plt.imshow(img1,cmap='gray') plt.title('pred: ' + str(np.argmax(out_pred[0])),fontsize=22) plt.figure('img 2') plt.imshow(img2,cmap='gray') plt.title('pred: ' + str(np.argmax(out_pred[1])),fontsize=22) plt.show() ","date":"2022-04-04","objectID":"/jetson_nano_tensorrt/:4:0","tags":["Article","Jetson Nano"],"title":"Jetson Nano TensorRT","uri":"/jetson_nano_tensorrt/"},{"categories":null,"content":"This is a guide for training and performing inference on Jetson Nano. ","date":"2022-03-10","objectID":"/jetson_nano_ai_fundamental/:0:0","tags":["Article","Jetson Nano"],"title":"Jetson Nano AI Fundamental","uri":"/jetson_nano_ai_fundamental/"},{"categories":null,"content":"Using ImageNet on Jetson git clone --recursive https://github.com/dusty-nv/jetson-inference cd jetson-inference docker/run.sh # Click OK to download pre-trained models #docker cd build/aarch64/bin ./imagenet \"images/object_*.jpg\" \"images/test/object_%i.jpg\" ./imagenet \"images/cat_*.jpg\" \"images/test/cat_%i.jpg\" ","date":"2022-03-10","objectID":"/jetson_nano_ai_fundamental/:1:0","tags":["Article","Jetson Nano"],"title":"Jetson Nano AI Fundamental","uri":"/jetson_nano_ai_fundamental/"},{"categories":null,"content":"Training Keras Model and Perform Inference Download MNIST dataset and copy it to “dataset/mnist”. ","date":"2022-03-10","objectID":"/jetson_nano_ai_fundamental/:2:0","tags":["Article","Jetson Nano"],"title":"Jetson Nano AI Fundamental","uri":"/jetson_nano_ai_fundamental/"},{"categories":null,"content":"Read Input Images # import the needed libraries import numpy as np from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Activation from tensorflow.keras import backend as K from tensorflow.keras.preprocessing.image import ImageDataGenerator # config img_width, img_height = 28,28 #width \u0026 height of input image input_depth = 1 #1: gray image train_data_dir = 'dataset/mnist/training' #data training path testing_data_dir = 'dataset/mnist/testing' #data testing path epochs = 2 #number of training epoch batch_size = 5 #training batch size # define image generator for Keras, # here, we map pixel intensity to 0-1 train_datagen = ImageDataGenerator(rescale=1/255) test_datagen = ImageDataGenerator(rescale=1/255) # read image batch by batch train_generator = train_datagen.flow_from_directory( train_data_dir, color_mode='grayscale',#inpput iameg: gray target_size=(img_width,img_height),#input image size batch_size=batch_size,#batch size class_mode='categorical')#categorical: one-hot encoding format class label testing_generator = test_datagen.flow_from_directory( testing_data_dir, color_mode='grayscale', target_size=(img_width,img_height), batch_size=batch_size, class_mode='categorical') ","date":"2022-03-10","objectID":"/jetson_nano_ai_fundamental/:2:1","tags":["Article","Jetson Nano"],"title":"Jetson Nano AI Fundamental","uri":"/jetson_nano_ai_fundamental/"},{"categories":null,"content":"Define the Network # define number of filters and nodes in the fully connected layer NUMB_FILTER_L1 = 20 NUMB_FILTER_L2 = 20 NUMB_FILTER_L3 = 20 NUMB_NODE_FC_LAYER = 10 #define input image order shape if K.image_data_format() == 'channels_first': input_shape_val = (input_depth, img_width, img_height) else: input_shape_val = (img_width, img_height, input_depth) #define the network model = Sequential() # Layer 1 model.add(Conv2D(NUMB_FILTER_L1, (5, 5), input_shape=input_shape_val, padding='same', name='input_tensor')) model.add(Activation('relu')) model.add(MaxPool2D((2, 2))) # Layer 2 model.add(Conv2D(NUMB_FILTER_L2, (5, 5), padding='same')) model.add(Activation('relu')) model.add(MaxPool2D((2, 2))) # Layer 3 model.add(Conv2D(NUMB_FILTER_L3, (5, 5), padding='same')) model.add(Activation('relu')) # flattening the model for fully connected layer model.add(Flatten()) # fully connected layer model.add(Dense(NUMB_NODE_FC_LAYER, activation='relu')) # output layer model.add(Dense(train_generator.num_classes, activation='softmax', name='output_tensor')) # Compilile the network model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) # Show the model summary model.summary() ","date":"2022-03-10","objectID":"/jetson_nano_ai_fundamental/:2:2","tags":["Article","Jetson Nano"],"title":"Jetson Nano AI Fundamental","uri":"/jetson_nano_ai_fundamental/"},{"categories":null,"content":"Train the Network # Train and test the network model.fit_generator( train_generator,#our training generator #number of iteration per epoch = number of data / batch size steps_per_epoch=np.floor(train_generator.n/batch_size), epochs=epochs,#number of epoch validation_data=testing_generator,#our validation generator #number of iteration per epoch = number of data / batch size validation_steps=np.floor(testing_generator.n / batch_size)) ","date":"2022-03-10","objectID":"/jetson_nano_ai_fundamental/:2:3","tags":["Article","Jetson Nano"],"title":"Jetson Nano AI Fundamental","uri":"/jetson_nano_ai_fundamental/"},{"categories":null,"content":"Save the Trained Network print(\"Training is done!\") model.save('./model/modelLeNet5.h5') print(\"Model is successfully stored!\") ","date":"2022-03-10","objectID":"/jetson_nano_ai_fundamental/:2:4","tags":["Article","Jetson Nano"],"title":"Jetson Nano AI Fundamental","uri":"/jetson_nano_ai_fundamental/"},{"categories":null,"content":"Perform Inference # import the needed libraries from tensorflow.keras.models import load_model from PIL import Image from matplotlib import pyplot as plt import numpy as np # read the input image using Pillow (you can use another library, e.g., OpenCV) img1= Image.open(\"dataset/mnist/testing/0/img_108.jpg\") img2= Image.open(\"dataset/mnist/testing/1/img_0.jpg\") # convert to ndarray numpy img1 = np.asarray(img1) img2 = np.asarray(img2) # load the trained model model = load_model('./model/modelLeNet5.h5') # predict the input image using the loaded model pred1 = model.predict_classes((img1/255).reshape((1,28,28,1))) pred2 = model.predict_classes((img2/255).reshape((1,28,28,1))) # plot the prediction result plt.figure('img1') plt.imshow(img1,cmap='gray') plt.title('pred:'+str(pred1[0]), fontsize=22) plt.figure('img2') plt.imshow(img2,cmap='gray') plt.title('pred:'+str(pred2[0]), fontsize=22) plt.show() ","date":"2022-03-10","objectID":"/jetson_nano_ai_fundamental/:2:5","tags":["Article","Jetson Nano"],"title":"Jetson Nano AI Fundamental","uri":"/jetson_nano_ai_fundamental/"},{"categories":null,"content":"Reference NVIDIA - Jetson Inference Ardian Uman - Tenorflow-TensorRT ","date":"2022-03-10","objectID":"/jetson_nano_ai_fundamental/:3:0","tags":["Article","Jetson Nano"],"title":"Jetson Nano AI Fundamental","uri":"/jetson_nano_ai_fundamental/"},{"categories":null,"content":"This is a Quick Start for Jetson Nano. Board used in this article is Jetson Nano 2GB. ","date":"2022-03-02","objectID":"/jetson_nano_setup/:0:0","tags":["Article","Jetson Nano"],"title":"Jetson Nano Setup","uri":"/jetson_nano_setup/"},{"categories":null,"content":"Setup ","date":"2022-03-02","objectID":"/jetson_nano_setup/:1:0","tags":["Article","Jetson Nano"],"title":"Jetson Nano Setup","uri":"/jetson_nano_setup/"},{"categories":null,"content":"Overview Jetson Nano 2GB\" Jetson Nano 2GB Ports ① microSD card slot for main storage ⑥ USB 3.0 port (x1) ② 40-pin expansion header ⑦ HDMI output port ③ Micro-USB port for Device Mode ⑧ USB-C for 5V power input ④ Gigabit Ethernet port ⑨ MIPI CSI-2 camera connector ⑤ USB 2.0 ports (x2) ","date":"2022-03-02","objectID":"/jetson_nano_setup/:1:1","tags":["Article","Jetson Nano"],"title":"Jetson Nano Setup","uri":"/jetson_nano_setup/"},{"categories":null,"content":"Write Image to microSD Card Download Jetson Nano 2GB Developer Kit SD Card Image Write image to microSD with Etcher ","date":"2022-03-02","objectID":"/jetson_nano_setup/:1:2","tags":["Article","Jetson Nano"],"title":"Jetson Nano Setup","uri":"/jetson_nano_setup/"},{"categories":null,"content":"First Boot Insert the microSD card Set the developer kit on a non-conductive surface Connect monitor, keyboard, mouse and USB-C power supply (5V⎓3A) Boot and setup ","date":"2022-03-02","objectID":"/jetson_nano_setup/:1:3","tags":["Article","Jetson Nano"],"title":"Jetson Nano Setup","uri":"/jetson_nano_setup/"},{"categories":null,"content":"Pytorch ","date":"2022-03-02","objectID":"/jetson_nano_setup/:2:0","tags":["Article","Jetson Nano"],"title":"Jetson Nano Setup","uri":"/jetson_nano_setup/"},{"categories":null,"content":"PyTorch pip wheels Will install PyTorch v1.10.0 since the image is embedded with JetPack 4.6. ","date":"2022-03-02","objectID":"/jetson_nano_setup/:2:1","tags":["Article","Jetson Nano"],"title":"Jetson Nano Setup","uri":"/jetson_nano_setup/"},{"categories":null,"content":"Installation Install pip3 sudo apt install python3-pip Install Pytorch wget https://nvidia.box.com/shared/static/fjtbno0vpo676a25cgvuqc1wty0fkkg6.whl -O torch-1.10.0-cp36-cp36m-linux_aarch64.whl sudo apt-get install python3-pip libopenblas-base libopenmpi-dev pip3 install Cython pip3 install numpy torch-1.10.0-cp36-cp36m-linux_aarch64.whl Install torchvision sudo apt-get install libjpeg-dev zlib1g-dev libpython3-dev libavcodec-dev libavformat-dev libswscale-dev git clone --branch v0.11.1 https://github.com/pytorch/vision torchvision cd torchvision export BUILD_VERSION=0.11.1 python3 setup.py install --user cd ../ ","date":"2022-03-02","objectID":"/jetson_nano_setup/:2:2","tags":["Article","Jetson Nano"],"title":"Jetson Nano Setup","uri":"/jetson_nano_setup/"},{"categories":null,"content":"Verification python3 import torch print(torch.__version__) print('CUDA available: ' + str(torch.cuda.is_available())) print('cuDNN version: ' + str(torch.backends.cudnn.version())) a = torch.cuda.FloatTensor(2).zero_() print('Tensor a = ' + str(a)) b = torch.randn(2).cuda() print('Tensor b = ' + str(b)) c = a + b print('Tensor c = ' + str(c)) import torchvision print(torchvision.__version__) ","date":"2022-03-02","objectID":"/jetson_nano_setup/:2:3","tags":["Article","Jetson Nano"],"title":"Jetson Nano Setup","uri":"/jetson_nano_setup/"},{"categories":null,"content":"Tensorflow ","date":"2022-03-02","objectID":"/jetson_nano_setup/:3:0","tags":["Article","Jetson Nano"],"title":"Jetson Nano Setup","uri":"/jetson_nano_setup/"},{"categories":null,"content":"Installation sudo apt-get update sudo apt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran sudo apt-get install python3-pip pip3 install -U --no-deps numpy==1.19.4 future==0.18.2 mock==3.0.5 keras_preprocessing==1.1.2 keras_applications==1.0.8 gast==0.4.0 protobuf pybind11 cython pkgconfig sudo env H5PY_SETUP_REQUIRES=0 pip3 install -U h5py==3.1.0 pip3 install -U pip testresources setuptools==49.6.0 pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v46 tensorflow ","date":"2022-03-02","objectID":"/jetson_nano_setup/:3:1","tags":["Article","Jetson Nano"],"title":"Jetson Nano Setup","uri":"/jetson_nano_setup/"},{"categories":null,"content":"Verification python3 import tensorflow ","date":"2022-03-02","objectID":"/jetson_nano_setup/:3:2","tags":["Article","Jetson Nano"],"title":"Jetson Nano Setup","uri":"/jetson_nano_setup/"},{"categories":null,"content":"Reference Zhihu - Jetson Nano 快速入门 NVIDIA Developer - Getting Started with Jetson Nano 2GB Developer Kit ","date":"2022-03-02","objectID":"/jetson_nano_setup/:4:0","tags":["Article","Jetson Nano"],"title":"Jetson Nano Setup","uri":"/jetson_nano_setup/"},{"categories":null,"content":"Welcome. This is the first post of my blog. Hugo Setup: Cyhour My Theme: LoveIt ","date":"2022-03-01","objectID":"/hello_world/:0:0","tags":["Blog"],"title":"Hello World","uri":"/hello_world/"}]